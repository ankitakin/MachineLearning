
Programming assignment 11: Gaussian Mixture Model
In [1]:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import seaborn as sns
sns.set_style('whitegrid')
%matplotlib inline

from scipy.stats import multivariate_normal

Your task

In this homework sheet we will implement Expectation-Maximization algorithm for learning & inference in a Gaussian mixture model.

We will use the dataset containing information about eruptions of a geyser called "Old Faithful". The dataset in suitable format can be downloaded from Piazza.

As usual, your task is to fill out the missing code, run the notebook, convert it to PDF and attach it you your HW solution.
Generate and visualize the data
In [2]:

X = np.loadtxt('faithful.txt')
plt.figure(figsize=[6, 6])
plt.scatter(X[:, 0], X[:, 1])
plt.xlabel('Eruptions (minutes)')
plt.ylabel('Waiting time (minutes)')
plt.show()

Task 1: Normalize the data

Notice, how the values on two axes are on very different scales. This might cause problems for our clustering algorithm.

Normalize the data, such that it lies in the range $[0, 1]$ along each dimension (each column of X).
In [3]:

def normalize_data(X):
    """Normalize data such that it lies in range [0, 1] along every dimension.
    
    Parameters
    ----------
    X : np.array, shape [N, D]
        Data matrix, each row represents a sample.
        
    Returns
    -------
    X_norm : np.array, shape [N, D]
        Normalized data matrix. 
    """
    mins = np.min(X, axis=0)
    maxs = np.max(X, axis=0)
    rng = maxs - mins
    X_norm = 1 - (((1 - 0) * (maxs - X)) / rng)
    return X_norm

In [4]:

plt.figure(figsize=[6, 6])
X_norm = normalize_data(X)
plt.scatter(X_norm[:, 0], X_norm[:, 1]);

Task 2: Compute the log-likelihood of GMM

Here and in some other places, you might want to use the function multivariate_normal.pdf from the scipy.stats package.
In [5]:

def gmm_log_likelihood(X, means, covs, mixing_coefs):
    """Compute the log-likelihood of the data under current parameters setting.
    
    Parameters
    ----------
    X : np.array, shape [N, D]
        Data matrix with samples as rows.
    means : np.array, shape [K, D]
        Means of the GMM (\mu in lecture notes).
    covs : np.array, shape [K, D, D]
        Covariance matrices of the GMM (\Sigma in lecuture notes).
    mixing_coefs : np.array, shape [K]
        Mixing proportions of the GMM (\pi in lecture notes).
        
    Returns
    -------
    log_likelihood : float
        log p(X | \mu, \Sigma, \pi) - Log-likelihood of the data under the given GMM.
    """
    print('mixing_coefs',mixing_coefs.shape)
    print('means shape',means.shape)
    print('covs shape',covs.shape)
    #y = multivariate_normal.pdf(X, mean=means.flatten(), cov=covs.flatten(),allow_singular=True)
    y1 = multivariate_normal.pdf(X, mean=means[0], cov=covs[0],allow_singular=True)
    y2 = multivariate_normal.pdf(X, mean=means[1], cov=covs[1],allow_singular=True)
    y = np.vstack((y1,y2))
    log_likelihood = np.sum(np.log(np.matmul(y.T,mixing_coefs)))
    return log_likelihood

Task 3: E step
In [6]:

def e_step(X, means, covs, mixing_coefs):
    """Perform the E step.
    
    Compute the responsibilities.
    
    Parameters
    ----------
    X : np.array, shape [N, D]
        Data matrix with samples as rows.
    means : np.array, shape [K, D]
        Means of the GMM (\mu in lecture notes).
    covs : np.array, shape [K, D, D]
        Covariance matrices of the GMM (\Sigma in lecuture notes).
    mixing_coefs : np.array, shape [K]
        Mixing proportions of the GMM (\pi in lecture notes).
    
    Returns
    -------
    responsibilities : np.array, shape [N, K]
        Cluster responsibilities for the given data.
    """
    print('mixing_coefs',mixing_coefs.shape)
    num1 = mixing_coefs[0] * multivariate_normal.pdf(X, mean=means[0], cov=covs[0])
    num2 = mixing_coefs[1] * multivariate_normal.pdf(X, mean=means[1], cov=covs[1])
    num = np.vstack((num1,num2))
    deno1 = multivariate_normal.pdf(X, mean=means[0], cov=covs[0])
    deno2 = multivariate_normal.pdf(X, mean=means[1], cov=covs[1])
    deno = np.vstack((deno1,deno2))
    deno_mul = np.sum(np.matmul(deno.T,mixing_coefs))
    responsibilities = num/deno_mul
    print('num:',num.shape)
    print('deno_mul:',deno_mul.shape)
    print('responsibilities:',responsibilities.shape)
    return responsibilities.T

Task 4: M step
In [7]:

def m_step(X, responsibilities):
    """Update the parameters \theta of the GMM to maximize E[log p(X, Z | \theta)].
    
    Parameters
    ----------
    X : np.array, shape [N, D]
        Data matrix with samples as rows.
    responsibilities : np.array, shape [N, K]
        Cluster responsibilities for the given data.
    
    Returns
    -------
    means : np.array, shape [K, D]
        Means of the GMM (\mu in lecture notes).
    covs : np.array, shape [K, D, D]
        Covariance matrices of the GMM (\Sigma in lecuture notes).
    mixing_coefs : np.array, shape [K]
        Mixing proportions of the GMM (\pi in lecture notes).
    
    """
    sigma = np.zeros((2,2,2))
    responsibilities /= np.expand_dims(np.sum(responsibilities,axis=1),1)
    Nk = np.sum(responsibilities,axis=0) #Bishop eq.9.27
    means = (np.dot(X.T,responsibilities)/Nk).T #Calculates Bishop eq.9.24
    for k in range(2):  #Calculates Bishop eq.9.25
        Xm = X - means[k]
        sigma[k] = 1/Nk[k]*np.dot(Xm.T*responsibilities[:,k],Xm)
        assert np.all(np.linalg.eigvals(sigma[k]) > 0)
    mixing_coefs = Nk/np.sum(Nk)  #Calculates Bishop eq.9.26

    return means, sigma, mixing_coefs

Visualize the result (nothing to do here)
In [8]:

def plot_gmm_2d(X, responsibilities, means, covs, mixing_coefs):
    """Visualize a mixture of 2 bivariate Gaussians.
    
    This is badly written code. Please don't write code like this.
    """
    plt.figure(figsize=[6, 6])
    palette = np.array(sns.color_palette('colorblind', n_colors=3))[[0, 2]]
    colors = responsibilities.dot(palette)
    # Plot the samples colored according to p(z|x)
    plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.5)
    # Plot locations of the means
    for ix, m in enumerate(means):
        plt.scatter(m[0], m[1], s=300, marker='X', c=palette[ix],
                    edgecolors='k', linewidths=1,)
    # Plot contours of the Gaussian
    x = np.linspace(0, 1, 50)
    y = np.linspace(0, 1, 50)
    xx, yy = np.meshgrid(x, y)
    for k in range(len(mixing_coefs)):
        zz = mlab.bivariate_normal(xx, yy, np.sqrt(covs[k][0, 0]),
                                   np.sqrt(covs[k][1, 1]), 
                                   means[k][0], means[k][1], covs[k][0, 1])
        plt.contour(xx, yy, zz, 2, colors='k')
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.show()

Run the EM algorithm
In [9]:

X_norm = normalize_data(X)
max_iters = 20

# Initialize the parameters
means = np.array([[0.2, 0.6], [0.8, 0.4]])
covs =np.array([0.5 * np.eye(2),0.5 * np.eye(2)])
mixing_coefs = np.array([0.5, 0.5])

old_log_likelihood = gmm_log_likelihood(X_norm, means, covs, mixing_coefs)
responsibilities = e_step(X_norm, means, covs, mixing_coefs)
print('At initialization: log-likelihood = {0}'
      .format(old_log_likelihood))
plot_gmm_2d(X_norm, responsibilities, means, covs, mixing_coefs)

# Perform the EM iteration
for i in range(max_iters):
    responsibilities = e_step(X_norm, means, covs, mixing_coefs)
    means, covs, mixing_coefs = m_step(X_norm, responsibilities)
    new_log_likelihood = gmm_log_likelihood(X_norm, means, covs, mixing_coefs)
    # Report & visualize the optimization progress
    print('Iteration {0}: log-likelihood = {1:.2f}, improvement = {2:.2f}'
          .format(i, new_log_likelihood, new_log_likelihood - old_log_likelihood))
    old_log_likelihood = new_log_likelihood
    plot_gmm_2d(X_norm, responsibilities, means, covs, mixing_coefs)

mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
At initialization: log-likelihood = -382.7055152420654

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 0: log-likelihood = 131.29, improvement = 513.99

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 1: log-likelihood = 131.48, improvement = 0.19

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 2: log-likelihood = 131.75, improvement = 0.27

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 3: log-likelihood = 132.15, improvement = 0.40

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 4: log-likelihood = 132.77, improvement = 0.62

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 5: log-likelihood = 133.81, improvement = 1.04

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 6: log-likelihood = 135.74, improvement = 1.93

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 7: log-likelihood = 139.88, improvement = 4.14

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 8: log-likelihood = 150.67, improvement = 10.79

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 9: log-likelihood = 181.12, improvement = 30.45

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 10: log-likelihood = 220.93, improvement = 39.81

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 11: log-likelihood = 234.06, improvement = 13.14

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 12: log-likelihood = 244.83, improvement = 10.77

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 13: log-likelihood = 258.67, improvement = 13.84

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 14: log-likelihood = 272.91, improvement = 14.23

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 15: log-likelihood = 284.29, improvement = 11.38

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 16: log-likelihood = 289.94, improvement = 5.65

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 17: log-likelihood = 290.39, improvement = 0.45

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 18: log-likelihood = 290.41, improvement = 0.01

mixing_coefs (2,)
num: (2, 272)
deno_mul: ()
responsibilities: (2, 272)
mixing_coefs (2,)
means shape (2, 2)
covs shape (2, 2, 2)
Iteration 19: log-likelihood = 290.41, improvement = 0.00

